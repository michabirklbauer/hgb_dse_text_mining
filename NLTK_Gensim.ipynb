{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d49afba-1ae8-4b99-adfb-4e3f0789bbac",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95502191-b13c-4fea-bbcc-69f9d29e5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "# english data\n",
    "classes_en = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tech\"}\n",
    "train_en = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/AGNews/train.csv\", \n",
    "                       names = [\"Label\", \"Title\", \"Article\"],\n",
    "                       encoding = \"utf-8\")\n",
    "test_en = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/AGNews/test.csv\", \n",
    "                      names = [\"Label\", \"Title\", \"Article\"],\n",
    "                      encoding = \"utf-8\")\n",
    "\n",
    "# german data\n",
    "train_de = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/10kGNAD/train.csv\", \n",
    "                       sep = \";\", names = [\"Label\", \"Article\"], \n",
    "                       quotechar = \"\\'\", quoting = csv.QUOTE_MINIMAL, encoding = \"utf-8\")\n",
    "test_de = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/10kGNAD/test.csv\", \n",
    "                       sep = \";\", names = [\"Label\", \"Article\"], \n",
    "                       quotechar = \"\\'\", quoting = csv.QUOTE_MINIMAL, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13421461-7335-4c46-9b9f-2d4e134eea2a",
   "metadata": {},
   "source": [
    "By iterating over the dataframe columns we can construct a \"vanilla\" list of documents that we can work on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3169c-0449-41c2-9535-68b7a3f5aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_en = [classes_en[int(row[\"Label\"])] for i, row in train_en.iterrows()]\n",
    "articles_en = [row[\"Article\"] for i, row in train_en.iterrows()]\n",
    "labels_de = [row[\"Label\"] for i, row in train_de.iterrows()]\n",
    "articles_de = [row[\"Article\"] for i, row in train_de.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3a938-d719-4300-925b-7734d9480567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles_en[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b7ddb-1d99-45fd-b9d5-e4e70db3e35a",
   "metadata": {},
   "source": [
    "# **NLTK**\n",
    "\n",
    "[https://www.nltk.org/](https://www.nltk.org/)\n",
    "\n",
    "NLTK - short for Natural Language Toolkit - is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning and wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "I mostly use NLTK for preprocessing tasks because it is more light-weight and straight forward than spaCy in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54b783-e6f0-4c64-890e-d303a5b8817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as nltkStopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a51cd-3006-4373-b0ce-1e4a5db5b3ac",
   "metadata": {},
   "source": [
    "### NLTK tokenizes documents which are any string variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d409dac-d43a-43a4-ada9-554182772014",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "articles_en_tokenized = [nltk.word_tokenize(doc) for doc in articles_en]\n",
    "articles_de_tokenized = [nltk.word_tokenize(doc) for doc in articles_de]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb87440-321d-458e-a516-4aac1bf15688",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_en_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408f4b3-ed37-4c25-b4ad-3e03e0fbedcb",
   "metadata": {},
   "source": [
    "### Stemming can be done with NLTK's Snowball Stemmer\n",
    "\n",
    "[https://www.nltk.org/api/nltk.stem.snowball.html](https://www.nltk.org/api/nltk.stem.snowball.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d8815-e62b-4fb4-b013-d3339df493e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokenized_document: str, language: str | None = None) -> List[str]:\n",
    "        stemmer = SnowballStemmer(language, ignore_stopwords = False)\n",
    "        return [stemmer.stem(word) for word in tokenized_document]\n",
    "    \n",
    "articles_en_stemmed = [stem(doc, \"english\") for doc in articles_en_tokenized]\n",
    "articles_de_stemmed = [stem(doc, \"german\") for doc in articles_de_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b328f7-046b-4916-b83f-1802cd415a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_en_stemmed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eeedf6-ac68-4c4a-bf40-858bbaeb98ba",
   "metadata": {},
   "source": [
    "### NLTK also offers built-in stopword sets for different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb5fa9-d4f6-4924-8edc-ef0644013585",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords_en = set(nltkStopwords.words(\"english\"))\n",
    "stopwords_de = set(nltkStopwords.words(\"german\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf13239-77d4-4fbd-87bb-0bf83e39b2a6",
   "metadata": {},
   "source": [
    "### The english stopwords are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952154b1-b41f-450a-a956-324666630d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\",\".join(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17cfe54-5d57-4659-b2ca-47fefd178f3d",
   "metadata": {},
   "source": [
    "### And the german ones are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa15a4a-513c-419a-a06b-f9ab145bb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\",\".join(stopwords_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb452b-ba84-4c1c-8ba8-5066a2910167",
   "metadata": {},
   "source": [
    "### Removing stopwords from our stemmed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b0f60-11c7-4d59-b080-07e186f3238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(stemmed_document: str, stopwords: Set) -> List[str]:\n",
    "        def is_stopword(word):\n",
    "            return not word in stopwords\n",
    "        return list(filter(is_stopword, stemmed_document))\n",
    "    \n",
    "articles_en_final = [remove_stopwords(doc, stopwords_en) for doc in articles_en_stemmed]\n",
    "articles_de_final = [remove_stopwords(doc, stopwords_de) for doc in articles_de_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e1ce3-03f9-4c07-b5f0-8687c429ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_en_final[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400603ab-fd34-442f-83cb-bc6fa423515e",
   "metadata": {},
   "source": [
    "# **Gensim**\n",
    "\n",
    "[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)\n",
    "\n",
    "Gensim titles itself as \"Topic Modelling for Humans\" and is the third and final NLP library that we will have a look at. I have mainly used Gensim to build TF-IDF models and run text queries on datasets. We are going to use our NLTK preprocessed documents as input to build a dictionary, corpus and index with Gensim and calculate the TF-IDF matrix to run text queries on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da13794-2d08-46dc-920c-8acbbffff714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b53d3-6dcd-4b60-9639-2a112e161d7a",
   "metadata": {},
   "source": [
    "### Building the TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee423808-4843-41ef-92b5-7e930498e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500 # adjust if model too big\n",
    "corpus_dictionary_en = corpora.Dictionary(articles_en_final[:size])\n",
    "corpus_en = [corpus_dictionary_en.doc2bow(document) for document in articles_en_final[:size]]\n",
    "model_en = models.TfidfModel(corpus_en)\n",
    "index_en = similarities.MatrixSimilarity(model_en[corpus_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3234d6-ee85-4fd2-b516-5db5a9f49bb6",
   "metadata": {},
   "source": [
    "To calculate the similarity of our input the query has to be preprocessed the same way our data was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0c310-f226-47f7-bb96-536865b5d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_en(query_string: str) -> List[Tuple[int, float]]:\n",
    "    q = corpus_dictionary_en.doc2bow(remove_stopwords(stem(nltk.word_tokenize(query_string), language = \"english\"), stopwords_en))\n",
    "    q_model = model_en[q]\n",
    "    result = index_en[q_model]\n",
    "    result = sorted(enumerate(result), key = lambda item: -item[1])\n",
    "    for i, j in enumerate(result):\n",
    "        if i > 2:\n",
    "            break\n",
    "        print(j, articles_en[:size][j[0]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c3769-8a3b-4ab5-b55a-94fdb159863c",
   "metadata": {},
   "source": [
    "### Gensim returns the resulting document and its similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1848f-1f64-4c14-81e9-a9673b508c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_en(\"Scientists United States\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65b34f-6c0d-499c-b870-05605168623c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
